{
 "cells": [
  {
   "cell_type": "code",
   "id": "xpxauz3tnjf",
   "source": "# Add src directory to Python path for module imports\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath('../src'))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a739bb-8573-4997-b8ae-ab1fe0d1787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "from config import NYUV2, MAKE3D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aaae07-cd40-47ab-86a5-e83d74470c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: set dataset, split, and paths\n",
    "split = 'test'\n",
    "DATASET = \"NYUv2\"\n",
    "\n",
    "\n",
    "if DATASET == \"NYUv2\":\n",
    "    PARENT = NYUV2.experiment_folder\n",
    "    data_dir = os.path.join(NYUV2.data_dir, \"NYUv2\", split + \"_rgb\")\n",
    "    file_extension = '.png'\n",
    "    out_csv = \"summary_metrics_all_\"+split+\"_nyuv2.csv\"\n",
    "    exp_name_format = \"nyuv2-*\" # optional \n",
    "elif DATASET == \"Make3D\":\n",
    "    PARENT = MAKE3D.experiment_folder\n",
    "    data_dir = os.path.join(MAKE3D.data_dir, \"Make3D\", \"Train400Img\" if split == \"train\" else \"Test134Img\")\n",
    "    file_extension = '.jpg'\n",
    "    out_csv = \"summary_metrics_all_\"+split+\"_make3d.csv\"\n",
    "    c1_out_csv = \"summary_metrics_all_\"+split+\"_make3d_c1_error.csv\"\n",
    "    exp_name_format = \"make3d-*\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb8ddb9-0ad7-4030-8dae-d2f505b8564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for parsing metrics and writing CSV\n",
    "\n",
    "def parse_last_loss(losses_path):\n",
    "    # todo: this reads whole file not super efficient\n",
    "    with open(losses_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip(): # not blank\n",
    "                last = line.strip()\n",
    "    return float(last)\n",
    "\n",
    "def parse_accuracy_metrics(metrics_path):\n",
    "    rms = rel = d1 = d2 = d3 = None\n",
    "    with open(metrics_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.lower().startswith(\"rms\"):\n",
    "                rms = float(line.split(\":\")[1].strip())\n",
    "            elif line.lower().startswith(\"rel\"):\n",
    "                rel = float(line.split(\":\")[1].strip())\n",
    "            elif line.lower().startswith(\"accuracy\"):\n",
    "                right = line.split(\":\")[1].strip()\n",
    "                d1, d2, d3 = [float(x.strip()) for x in right.split(\",\")]\n",
    "    return (rms, rel, d1, d2, d3)\n",
    "\n",
    "def write_csv_with_average(rows, fieldnames, output_csv):\n",
    "    \"\"\"Write rows to CSV, adding an average row and sorting by run_id.\"\"\"\n",
    "    rows.sort(key=lambda r: r[\"run_id\"])\n",
    "    \n",
    "    avg_row = {\"run_id\": \"AVERAGE\"}\n",
    "    for col in fieldnames[1:]:\n",
    "        values = [r[col] for r in rows if r[col] is not None]\n",
    "        avg_row[col] = sum(values) / len(values) if values else None\n",
    "    \n",
    "    rows.append(avg_row)\n",
    "    \n",
    "    with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "    \n",
    "    print(f\"Wrote {len(rows)} rows to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51d44fb-ab5f-4d58-b002-a958ddff9f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of experiment directories\n",
    "\n",
    "# # Alternatively, look for specific file pattern\n",
    "# pattern = os.path.join(PARENT, exp_name_format)\n",
    "# dirs = [d for d in glob.glob(pattern) if os.path.isdir(d)] \n",
    "\n",
    "dirs = [d for d in os.listdir(PARENT) if os.path.isdir(os.path.join(PARENT, d))]\n",
    "\n",
    "# Rremove repeats + sort\n",
    "dirs = sorted(set(dirs))\n",
    "print(len(dirs))\n",
    "\n",
    "# Assert expected number of files\n",
    "num_files = len(dirs)\n",
    "expected_num_files = len([f for f in os.listdir(data_dir) if f.endswith(file_extension)])\n",
    "assert num_files == expected_num_files, \"Expected {expected_num_files} files, but found {num_files} files\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5905f5e8-a7f6-4202-aca5-384699791829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN WORKFLOW: Compile accuracy metrics from experiment results\n",
    "# Reads losses.txt and accuracy_metrics.txt from each experiment directory\n",
    "# Outputs: RMS, Rel, delta1, delta2, delta3, last_loss\n",
    "\n",
    "rows = []\n",
    "\n",
    "for d in dirs:\n",
    "    run_id = os.path.basename(d).split(\"_\")[0].split(\"-\")[-1]\n",
    "\n",
    "    losses_path = os.path.join(PARENT, d, \"losses.txt\")\n",
    "    metrics_path = os.path.join(PARENT, d, \"accuracy_metrics.txt\")\n",
    "\n",
    "    if not os.path.exists(losses_path) or not os.path.exists(metrics_path):\n",
    "        continue\n",
    "\n",
    "    last_loss = parse_last_loss(losses_path)\n",
    "    rms, rel, d1, d2, d3 = parse_accuracy_metrics(metrics_path)\n",
    "\n",
    "    rows.append({\n",
    "        \"run_id\": run_id,\n",
    "        \"last_loss\": last_loss,\n",
    "        \"RMS\": rms,\n",
    "        \"Rel\": rel,\n",
    "        \"delta1\": d1,\n",
    "        \"delta2\": d2,\n",
    "        \"delta3\": d3,\n",
    "    })\n",
    "\n",
    "fieldnames = [\"run_id\", \"last_loss\", \"RMS\", \"Rel\", \"delta1\", \"delta2\", \"delta3\"]\n",
    "write_csv_with_average(rows, fieldnames, out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3970a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE3D ONLY: Compute C1 error metrics\n",
    "# Recomputes RMS and Rel from saved depth maps with C1 threshold (depth < 70)\n",
    "# Only run this cell if DATASET == \"Make3D\"\n",
    "\n",
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "rows = []\n",
    "\n",
    "for d in dirs:\n",
    "    run_id = os.path.basename(d).split(\"_\")[0].split(\"windowed5-\")[1]\n",
    "\n",
    "    dpt = utils.load_dpt_npy(os.path.join(PARENT, d), 'dpt')\n",
    "\n",
    "    # compute c1 error\n",
    "    _, gt_dpt = utils.load_single_sample_Make3D(img_name = \"img-\"+run_id+\".jpg\", data_dir = MAKE3D.data_dir,\n",
    "                                                split=split)\n",
    "\n",
    "    mask = (gt_dpt < 70) & np.isfinite(gt_dpt)\n",
    "    gt_dpt_masked = gt_dpt[mask]\n",
    "    dpt_masked = dpt[mask]\n",
    "    assert np.all(gt_dpt_masked < 70)\n",
    "\n",
    "    rms = utils.compute_RMS(dpt_masked, gt_dpt_masked)\n",
    "    rel = utils.compute_AbsRel(dpt_masked, gt_dpt_masked)\n",
    "\n",
    "    rows.append({\n",
    "        \"run_id\": run_id,\n",
    "        \"RMS\": rms,\n",
    "        \"Rel\": rel,\n",
    "    })\n",
    "\n",
    "fieldnames = [\"run_id\", \"RMS\", \"Rel\"]\n",
    "write_csv_with_average(rows, fieldnames, c1_out_csv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}